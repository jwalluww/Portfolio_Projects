{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc6ea9dc",
   "metadata": {},
   "source": [
    "# Pandas Alternatives\n",
    "##### “What is the average revenue per user by treatment group?”\n",
    "- Step 1: Load data\n",
    "- Step 2: Group + aggregate\n",
    "- Step 3: Convert to pandas\n",
    "- Step 4: Plot\n",
    "- Step 5: One sentence takeaway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "466e43b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "n = 10_000\n",
    "df = pd.DataFrame({\n",
    "    \"user_id\": np.arange(n),\n",
    "    \"treatment\": np.random.choice([\"control\", \"test\"], size=n),\n",
    "    \"revenue\": np.random.gamma(shape=2, scale=10, size=n)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa74d17",
   "metadata": {},
   "source": [
    "## Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be6e6305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas Summary:\n",
      "  treatment  avg_revenue\n",
      "0   control    20.083176\n",
      "1      test    20.063683\n"
     ]
    }
   ],
   "source": [
    "summary_pd = (\n",
    "    df\n",
    "    .groupby(\"treatment\", as_index=False)\n",
    "    .agg(avg_revenue=(\"revenue\", \"mean\"))\n",
    ")\n",
    "print(\"Pandas Summary:\")\n",
    "print(summary_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a31720",
   "metadata": {},
   "source": [
    "Normal every day pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818c0dde",
   "metadata": {},
   "source": [
    "## Polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7de6c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polars Summary:\n",
      "shape: (2, 2)\n",
      "┌───────────┬─────────────┐\n",
      "│ treatment ┆ avg_revenue │\n",
      "│ ---       ┆ ---         │\n",
      "│ str       ┆ f64         │\n",
      "╞═══════════╪═════════════╡\n",
      "│ control   ┆ 20.083176   │\n",
      "│ test      ┆ 20.063683   │\n",
      "└───────────┴─────────────┘\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "pl_df = pl.from_pandas(df)\n",
    "\n",
    "summary_pl = (\n",
    "    pl_df\n",
    "    .group_by(pl.col(\"treatment\"))\n",
    "    .agg(\n",
    "        pl.col(\"revenue\").mean().alias(\"avg_revenue\")\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Polars Summary:\")\n",
    "print(summary_pl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc69995",
   "metadata": {},
   "source": [
    "Feels like pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5909841b",
   "metadata": {},
   "source": [
    "## DuckDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6932834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DuckDB Summary:\n",
      "  treatment  avg_revenue\n",
      "0      test    20.063683\n",
      "1   control    20.083176\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "summary_duck = duckdb.sql(\"\"\"\n",
    "    SELECT\n",
    "        treatment,\n",
    "        AVG(revenue) AS avg_revenue\n",
    "    FROM df\n",
    "    GROUP BY treatment\n",
    "\"\"\").df()\n",
    "print(\"DuckDB Summary:\")\n",
    "print(summary_duck)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17437fba",
   "metadata": {},
   "source": [
    "This is legit sql that works directly off of a pandas df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0bf48c",
   "metadata": {},
   "source": [
    "## Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca10a64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask Summary:\n",
      "  treatment  avg_revenue\n",
      "0   control    20.083176\n",
      "1      test    20.063683\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "ddf = dd.from_pandas(df, npartitions=4)\n",
    "\n",
    "summary_dask = (\n",
    "    ddf\n",
    "    .groupby(\"treatment\")\n",
    "    .revenue\n",
    "    .mean()\n",
    "    .compute()\n",
    "    .reset_index(name=\"avg_revenue\")\n",
    ")\n",
    "print(\"Dask Summary:\")\n",
    "print(summary_dask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef18276b",
   "metadata": {},
   "source": [
    "Probably the least intuitive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e727dbf",
   "metadata": {},
   "source": [
    "### Installation issues with the remainder, lost interest to move forward, but here's a summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac68e70f",
   "metadata": {},
   "source": [
    "- Pandas for exploration\n",
    "- Polars for speed on one machine\n",
    "- DuckDB for local SQL analytics on parquet\n",
    "- Dask or Modin when scaling Pandas\n",
    "- Spark only when data requires a cluster\n",
    "- PyArrow sits underneath for efficient IO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe8a2e7",
   "metadata": {},
   "source": [
    "## PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dec70cd9",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o63.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 9 in stage 0.0 failed 1 times, most recent failure: Lost task 9.0 in stage 0.0 (TID 9) (DESKTOP-2OHFJED.lan executor driver): java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, The system cannot find the file specified\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1143)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1073)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:247)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:154)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:158)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:309)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:72)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:499)\r\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:158)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1110)\r\n\t... 35 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\r\n\tat scala.collection.immutable.List.foreach(List.scala:323)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\nCaused by: java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, The system cannot find the file specified\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1143)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1073)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:247)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:154)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:158)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:309)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:72)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:499)\r\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:158)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1110)\r\n\t... 35 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      4\u001b[39m spark = SparkSession.builder.getOrCreate()\n\u001b[32m      6\u001b[39m spark_df = spark.createDataFrame(df)\n\u001b[32m      8\u001b[39m summary_spark = (\n\u001b[32m      9\u001b[39m     \u001b[43mspark_df\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupBy\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtreatment\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mavg\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrevenue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mavg_revenue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m )\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSpark Summary:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(summary_spark)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wallj\\DS_Projects\\Portfolio_Projects\\py314_env\\Lib\\site-packages\\pyspark\\sql\\classic\\dataframe.py:1816\u001b[39m, in \u001b[36mDataFrame.toPandas\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1815\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtoPandas\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[33m\"\u001b[39m\u001b[33mPandasDataFrameLike\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1816\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPandasConversionMixin\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wallj\\DS_Projects\\Portfolio_Projects\\py314_env\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:188\u001b[39m, in \u001b[36mPandasConversionMixin.toPandas\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    185\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    187\u001b[39m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m188\u001b[39m rows = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(rows) > \u001b[32m0\u001b[39m:\n\u001b[32m    190\u001b[39m     pdf = pd.DataFrame.from_records(\n\u001b[32m    191\u001b[39m         rows, index=\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(rows)), columns=\u001b[38;5;28mself\u001b[39m.columns  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    192\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wallj\\DS_Projects\\Portfolio_Projects\\py314_env\\Lib\\site-packages\\pyspark\\sql\\classic\\dataframe.py:443\u001b[39m, in \u001b[36mDataFrame.collect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcollect\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> List[Row]:\n\u001b[32m    442\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m._sc):\n\u001b[32m--> \u001b[39m\u001b[32m443\u001b[39m         sock_info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    444\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wallj\\DS_Projects\\Portfolio_Projects\\py314_env\\Lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wallj\\DS_Projects\\Portfolio_Projects\\py314_env\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:263\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpy4j\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[32m    262\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m263\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    265\u001b[39m     converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wallj\\DS_Projects\\Portfolio_Projects\\py314_env\\Lib\\site-packages\\py4j\\protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    333\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o63.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 9 in stage 0.0 failed 1 times, most recent failure: Lost task 9.0 in stage 0.0 (TID 9) (DESKTOP-2OHFJED.lan executor driver): java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, The system cannot find the file specified\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1143)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1073)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:247)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:154)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:158)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:309)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:72)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:499)\r\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:158)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1110)\r\n\t... 35 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:3122)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3122)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3114)\r\n\tat scala.collection.immutable.List.foreach(List.scala:323)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3114)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1303)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1303)\r\n\tat scala.Option.foreach(Option.scala:437)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1303)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3397)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3328)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3317)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\nCaused by: java.io.IOException: Cannot run program \"python3\": CreateProcess error=2, The system cannot find the file specified\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1143)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1073)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:247)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:154)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:158)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:309)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:72)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:180)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:716)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:86)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:83)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:97)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:719)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:499)\r\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:158)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1110)\r\n\t... 35 more\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark_df = spark.createDataFrame(df)\n",
    "\n",
    "summary_spark = (\n",
    "    spark_df\n",
    "    .groupBy(\"treatment\")\n",
    "    .agg(avg(\"revenue\").alias(\"avg_revenue\"))\n",
    "    .toPandas()\n",
    ")\n",
    "print(\"Spark Summary:\")\n",
    "print(summary_spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dbbeb2",
   "metadata": {},
   "source": [
    "## Modin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91847925",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Please refer to installation documentation page to install an engine",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodin\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m mdf = \u001b[43mmpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m summary_modin = (\n\u001b[32m      6\u001b[39m     mdf\n\u001b[32m      7\u001b[39m     .groupby(\u001b[33m\"\u001b[39m\u001b[33mtreatment\u001b[39m\u001b[33m\"\u001b[39m, as_index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      8\u001b[39m     .agg(avg_revenue=(\u001b[33m\"\u001b[39m\u001b[33mrevenue\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m      9\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wallj\\DS_Projects\\Portfolio_Projects\\py314_env\\Lib\\site-packages\\modin\\core\\storage_formats\\pandas\\query_compiler_caster.py:1039\u001b[39m, in \u001b[36mwrap_function_in_argument_caster.<locals>.f_with_argument_casting\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1030\u001b[39m \u001b[38;5;66;03m# Before determining any automatic switches, we perform the following checks:\u001b[39;00m\n\u001b[32m   1031\u001b[39m \u001b[38;5;66;03m# 1. If the global AutoSwitchBackend configuration variable is set to False, do not switch.\u001b[39;00m\n\u001b[32m   1032\u001b[39m \u001b[38;5;66;03m# 2. If there's only one query compiler and it's pinned, do not switch.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1035\u001b[39m \u001b[38;5;66;03m# 4. If there are multiple query compilers, at least two of which are pinned to distinct\u001b[39;00m\n\u001b[32m   1036\u001b[39m \u001b[38;5;66;03m#    backends, raise a ValueError.\u001b[39;00m\n\u001b[32m   1038\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(input_query_compilers) == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1039\u001b[39m     input_backend = \u001b[43mBackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1040\u001b[39m     \u001b[38;5;66;03m# For nullary functions, we need to create a dummy query compiler\u001b[39;00m\n\u001b[32m   1041\u001b[39m     \u001b[38;5;66;03m# to calculate the cost of switching backends. We should only\u001b[39;00m\n\u001b[32m   1042\u001b[39m     \u001b[38;5;66;03m# create the dummy query compiler once per backend.\u001b[39;00m\n\u001b[32m   1043\u001b[39m     input_qc_for_pre_op_switch = _BACKEND_TO_EMPTY_QC[input_backend]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wallj\\DS_Projects\\Portfolio_Projects\\py314_env\\Lib\\site-packages\\modin\\config\\envvars.py:640\u001b[39m, in \u001b[36mBackend.get\u001b[39m\u001b[34m(cls)\u001b[39m\n\u001b[32m    631\u001b[39m backend_config_value = Backend._get_value_from_config()\n\u001b[32m    633\u001b[39m \u001b[38;5;66;03m# If Backend is in the OS's configuration, use the configured Backend\u001b[39;00m\n\u001b[32m    634\u001b[39m \u001b[38;5;66;03m# value. Otherwise, we need to figure out the Backend value based on\u001b[39;00m\n\u001b[32m    635\u001b[39m \u001b[38;5;66;03m# the Engine and StorageFormat values.\u001b[39;00m\n\u001b[32m    636\u001b[39m \u001b[38;5;28mcls\u001b[39m._value = (\n\u001b[32m    637\u001b[39m     backend_config_value\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m backend_config_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _UNSET\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.get_backend_for_execution(\n\u001b[32m--> \u001b[39m\u001b[32m640\u001b[39m         Execution(storage_format=StorageFormat.get(), engine=\u001b[43mEngine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    641\u001b[39m     )\n\u001b[32m    642\u001b[39m )\n\u001b[32m    644\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._value\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wallj\\DS_Projects\\Portfolio_Projects\\py314_env\\Lib\\site-packages\\modin\\config\\envvars.py:328\u001b[39m, in \u001b[36mEngine.get\u001b[39m\u001b[34m(cls)\u001b[39m\n\u001b[32m    317\u001b[39m backend_config_value = Backend._get_value_from_config()\n\u001b[32m    319\u001b[39m \u001b[38;5;66;03m# If Engine is in the OS's configuration, use the configured Engine value.\u001b[39;00m\n\u001b[32m    320\u001b[39m \u001b[38;5;66;03m# Otherwise, use the Backend config value if that exists. If it doesn't,\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[38;5;66;03m# fall back to the default Engine value.\u001b[39;00m\n\u001b[32m    322\u001b[39m \u001b[38;5;28mcls\u001b[39m._value = (\n\u001b[32m    323\u001b[39m     engine_config_value\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m engine_config_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _UNSET\n\u001b[32m    325\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m (\n\u001b[32m    326\u001b[39m         Backend.get_execution_for_backend(backend_config_value).engine\n\u001b[32m    327\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m backend_config_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _UNSET\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    329\u001b[39m     )\n\u001b[32m    330\u001b[39m )\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._value\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\wallj\\DS_Projects\\Portfolio_Projects\\py314_env\\Lib\\site-packages\\modin\\config\\envvars.py:267\u001b[39m, in \u001b[36mEngine._get_default\u001b[39m\u001b[34m(cls)\u001b[39m\n\u001b[32m    261\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m    262\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mPlease `pip install \u001b[39m\u001b[33m\"\u001b[39m\u001b[33munidist[mpi]\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m` to install compatible unidist on MPI \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    263\u001b[39m             + \u001b[33m\"\u001b[39m\u001b[33mversion \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    264\u001b[39m             + \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMIN_UNIDIST_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m).\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    265\u001b[39m         )\n\u001b[32m    266\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mUnidist\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m    268\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mPlease refer to installation documentation page to install an engine\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    269\u001b[39m )\n",
      "\u001b[31mImportError\u001b[39m: Please refer to installation documentation page to install an engine"
     ]
    }
   ],
   "source": [
    "import modin.pandas as mpd\n",
    "\n",
    "mdf = mpd.DataFrame(df)\n",
    "\n",
    "summary_modin = (\n",
    "    mdf\n",
    "    .groupby(\"treatment\", as_index=False)\n",
    "    .agg(avg_revenue=(\"revenue\", \"mean\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ee358e",
   "metadata": {},
   "source": [
    "## Vaex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f579057",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vaex'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvaex\u001b[39;00m\n\u001b[32m      2\u001b[39m vdf = vaex.from_pandas(df)\n\u001b[32m      3\u001b[39m summary_vaex = vdf.groupby(\u001b[33m\"\u001b[39m\u001b[33mtreatment\u001b[39m\u001b[33m\"\u001b[39m, agg=vaex.agg.mean(\u001b[33m\"\u001b[39m\u001b[33mrevenue\u001b[39m\u001b[33m\"\u001b[39m)).to_pandas_df()\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'vaex'"
     ]
    }
   ],
   "source": [
    "import vaex\n",
    "vdf = vaex.from_pandas(df)\n",
    "summary_vaex = vdf.groupby(\"treatment\", agg=vaex.agg.mean(\"revenue\")).to_pandas_df()\n",
    "summary_vaex.rename(columns={\"revenue_mean\": \"avg_revenue\"}, inplace=True)\n",
    "print(\"Vaex Summary:\")\n",
    "print(summary_vaex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c5d4a3",
   "metadata": {},
   "source": [
    "## PyArrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d924ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "\n",
    "table = pa.Table.from_pandas(df)\n",
    "\n",
    "df_roundtrip = table.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a6574d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pyarrow.compute' has no attribute 'group_by'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyarrow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpc\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m grouped = \u001b[43mpc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroup_by\u001b[49m(\n\u001b[32m      4\u001b[39m     table,\n\u001b[32m      5\u001b[39m     keys=[\u001b[33m\"\u001b[39m\u001b[33mtreatment\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      6\u001b[39m     aggregates={\u001b[33m\"\u001b[39m\u001b[33mavg_revenue\u001b[39m\u001b[33m\"\u001b[39m: (\u001b[33m\"\u001b[39m\u001b[33mrevenue\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m\"\u001b[39m)}\n\u001b[32m      7\u001b[39m )\n\u001b[32m      9\u001b[39m grouped.to_pandas()\n",
      "\u001b[31mAttributeError\u001b[39m: module 'pyarrow.compute' has no attribute 'group_by'"
     ]
    }
   ],
   "source": [
    "import pyarrow.compute as pc\n",
    "\n",
    "grouped = pc.group_by(\n",
    "    table,\n",
    "    keys=[\"treatment\"],\n",
    "    aggregates={\"avg_revenue\": (\"revenue\", \"mean\")}\n",
    ")\n",
    "\n",
    "grouped.to_pandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py314_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
