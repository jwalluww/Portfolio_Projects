{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Variational Autoencoders to Perform Data Augmentation for Bank Fraud Detection\n",
    "---\n",
    "\n",
    "üîç **Situation**:\n",
    "- A bank needs a fraud detection model but cannot share real customer transactions with your consulting team due to customer data sensitivity.\n",
    "\n",
    "üìå **Task**:\n",
    "- Generate synthetic banking transactions that mimic real-world fraud patterns.\n",
    "- Train a fraud detection model on the synthetic data and evaluate its performance.\n",
    "\n",
    "‚ú® **Action**: \n",
    "- Use Pytorch to build a Variational Autoencoder (VAE) to generate synthetic banking transactions.\n",
    "\n",
    "üìà **Result**:\n",
    "- blah\n",
    "\n",
    "‚úç **Author**: Justin Wall\n",
    "üìÖ **Updated**: 03/04/2025\n",
    "\"\"\"\n",
    "# ===================================================\n",
    "# --- Step 1: Generate Synthetic Transaction Data ---\n",
    "# ===================================================\n",
    "#%%\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "np.random.seed(42)\n",
    "num_samples = 10000\n",
    "\n",
    "# Features: Amount, Time, Merchant Type, Transaction Type, Fraud Label\n",
    "real_data = np.hstack([\n",
    "    np.random.normal(100, 50, (num_samples, 1)),  # Transaction Amount\n",
    "    np.random.normal(500, 200, (num_samples, 1)),  # Time of transaction\n",
    "    np.random.randint(1, 10, (num_samples, 1)),  # Merchant type\n",
    "    np.random.randint(0, 2, (num_samples, 1)),  # Transaction type (0 = Online, 1 = POS)\n",
    "    np.random.choice([0, 1], size=(num_samples, 1), p=[0.98, 0.02])  # Fraud Label\n",
    "])\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "data_tensor = torch.tensor(real_data, dtype=torch.float32)\n",
    "dataloader = DataLoader(TensorDataset(data_tensor), batch_size=64, shuffle=True)\n",
    "#%%\n",
    "\n",
    "# ===================================================\n",
    "# --- Step 2: Define the Variational Autoencoder (VAE) ---\n",
    "# ===================================================\n",
    "#%%\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, latent_dim * 2)  # Mean and log variance\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        params = self.encoder(x)\n",
    "        mu, log_var = params.chunk(2, dim=1)\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        z = mu + std * torch.randn_like(std)  # Sampling\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon, mu, log_var\n",
    "#%%\n",
    "\n",
    "# ===================================================\n",
    "# --- Step 3: Train the Variational Autoencoder (VAE) ---\n",
    "# ===================================================\n",
    "#%%\n",
    "input_dim = real_data.shape[1]\n",
    "hidden_dim = 16\n",
    "latent_dim = 8\n",
    "\n",
    "vae = VAE(input_dim, hidden_dim, latent_dim)\n",
    "optimizer = optim.Adam(vae.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "for epoch in range(20):  # Train for 20 epochs\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        x = batch[0]\n",
    "        x_recon, mu, log_var = vae(x)\n",
    "        \n",
    "        # VAE Loss = Reconstruction Loss + KL Divergence\n",
    "        recon_loss = loss_fn(x_recon, x)\n",
    "        kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        loss = recon_loss + kl_loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader):.4f}\")\n",
    "#%%\n",
    "\n",
    "# ===================================================\n",
    "# --- Step 4: Generate Synthetic Transactions ---\n",
    "# ===================================================\n",
    "#%%\n",
    "with torch.no_grad():\n",
    "    z_samples = torch.randn((5000, latent_dim))  # Generate 5,000 synthetic transactions\n",
    "    synthetic_data = vae.decoder(z_samples).numpy()\n",
    "\n",
    "# Step 5: Convert Generated Data to DataFrame\n",
    "synthetic_df = pd.DataFrame(synthetic_data, columns=[\"Amount\", \"Time\", \"Merchant Type\", \"Transaction Type\", \"Fraud Label\"])\n",
    "print(synthetic_df.head())\n",
    "#%%\n",
    "\n",
    "Anomaly Detection (e.g., Isolation Forest, DBSCAN)\n",
    "Recommendation Systems (e.g., Collaborative Filtering, Matrix Factorization)\n",
    "Time Series Forecasting (e.g., ARIMA, Prophet)\n",
    "Graph-Based Machine Learning (e.g., Node Classification, Link Prediction)\n",
    "Text Classification (e.g., Sentiment Analysis with Naive Bayes or Transformer models)\n",
    "Autoencoders for Anomaly Detection\n",
    "Fairness in Machine Learning (e.g., Demographic Parity, Equalized Odds)\n",
    "Contrastive Learning (e.g., SimCLR for self-supervised learning)\n",
    "Bayesian Optimization for Hyperparameter Tuning\n",
    "Neural Networks for Tabular Data (e.g., TabNet, NODE)\n",
    "Geospatial Analysis (e.g., Clustering stores by location, Voronoi diagrams)\n",
    "Generative AI (e.g., Variational Autoencoders for data augmentation)\n",
    "Federated Learning (e.g., Training models on distributed data)\n",
    "Explainability Techniques (e.g., SHAP, LIME, Counterfactual Explanations)\n",
    "Simulating Causal Effects (e.g., Synthetic Control Method, G-Computation)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
