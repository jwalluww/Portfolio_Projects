{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Causal Inferece on Pricing Strategy & Customer Demand using DoWhy & NetworkX\n",
    "---\n",
    "\n",
    "üîç **Situation**:\n",
    "- Create a synthetic dataset for a retail pricing problem where we analyze the causal effects of a price change on customer demand\n",
    "- Understand relationship between product price, advertising spend, competitor pricing, and customer demand\n",
    "\n",
    "üìå **Task**:\n",
    "- Create synthetic retail pricing dataset\n",
    "- Design a causal graph based on domain knowledge\n",
    "- Perform causal discovery using DoWhy\n",
    "\n",
    "‚ú® **Action**: \n",
    "- This methodology was valuable in determining the cause of a revenue drop\n",
    "\n",
    "üìà **Result**:\n",
    "- Run some more tests to determine causality using the DAG\n",
    "\n",
    "‚úç **Author**: Justin Wall\n",
    "üìÖ **Updated**: 03/04/2025\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================= #\n",
    "# Generate Fake Retail Pricing Data #\n",
    "# ================================= #\n",
    "#%%\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Number of observations\n",
    "n = 1000  \n",
    "\n",
    "# Generate features\n",
    "ad_spend = np.random.uniform(1000, 5000, n)  # Ad budget\n",
    "competitor_price = np.random.uniform(10, 50, n)  # Competitor pricing\n",
    "price = np.random.uniform(15, 55, n)  # Our product price\n",
    "\n",
    "# Define true causal mechanisms\n",
    "# Competitor price influences our price strategy\n",
    "price = price - 0.3 * competitor_price + np.random.normal(0, 2, n)\n",
    "\n",
    "# Ad spend influences both our price strategy and sales\n",
    "sales = 500 - 5 * price + 0.1 * ad_spend + 3 * competitor_price + np.random.normal(0, 10, n)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\"price\": price, \"ad_spend\": ad_spend, \"competitor_price\": competitor_price, \"sales\": sales})\n",
    "\n",
    "df.head()\n",
    "#%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================= #\n",
    "# Create Causal Graph               #\n",
    "# ================================= #\n",
    "#%%\n",
    "# Define causal relationships\n",
    "causal_graph = nx.DiGraph()\n",
    "causal_graph.add_edges_from([\n",
    "    (\"ad_spend\", \"sales\"),\n",
    "    (\"competitor_price\", \"price\"),\n",
    "    (\"competitor_price\", \"sales\"),\n",
    "    (\"price\", \"sales\")\n",
    "])\n",
    "# Draw up relationships using domain knowledge\n",
    "\n",
    "# Plot the causal graph\n",
    "plt.figure(figsize=(6, 4))\n",
    "pos = nx.spring_layout(causal_graph, seed=42)\n",
    "nx.draw(causal_graph, pos, with_labels=True, node_color=\"lightblue\", edge_color=\"black\", node_size=3000, font_size=12)\n",
    "plt.title(\"Causal Graph for Pricing & Customer Demand\")\n",
    "plt.show()\n",
    "#%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================= #\n",
    "# Perform Causal Discovery w/DoWhy  #\n",
    "# ================================= #\n",
    "#%%\n",
    "from dowhy import gcm\n",
    "from dowhy.gcm import InvertibleStructuralCausalModel, fit\n",
    "\n",
    "# Define causal model\n",
    "model = InvertibleStructuralCausalModel(causal_graph)\n",
    "\n",
    "# Assign causal mechanisms (Gaussian Processes for simplicity)\n",
    "gcm.auto.assign_causal_mechanisms(model, df)\n",
    "\n",
    "# Train causal model\n",
    "fit(model, df)\n",
    "#%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================= #\n",
    "# Evaluate Causal Graph             #\n",
    "# ================================= #\n",
    "#%%\n",
    "\n",
    "gcm.evaluate_causal_model(\n",
    "    model,\n",
    "    df,\n",
    "    compare_mechanism_baselines=True,\n",
    "    evaluate_invertibility_assumptions=True,\n",
    ")\n",
    "#%%\n",
    "# PNL Assumption for sales and price is (1.0, False, 0.05)\n",
    "# 1.0 ‚Üí Suggests that a nonlinear causal relationship might be necessary.\n",
    "# False ‚Üí Indicates that no strong violation of the assumption was detected.\n",
    "# 0.05 ‚Üí Significance level used for the test.\n",
    "# ‚úÖ Interpretation: Your causal model is not strongly rejecting the assumption, but the relationships may be somewhat nonlinear. If the model performance is poor, consider using nonparametric methods (e.g., Gaussian Processes) instead of linear models.\n",
    "# DoWhy tests 24 different DAG permutations to check if your causal graph is part of the Markov equivalence class (i.e., a valid structure).\n",
    "# 2/24 permutations were equivalent to your DAG.\n",
    "# p-value: 0.08 means there is no strong evidence against your graph, though it's not highly robust either.\n",
    "# ‚úÖ Interpretation: Your DAG is informative and likely close to the true causal structure, but there might be alternative DAGs that fit nearly as well.\n",
    "# No violations of Local Markov Conditions (0/4 LMCs) ‚Üí Your causal relationships follow conditional independence rules.\n",
    "# Your DAG is better than 91.7% of alternative DAGs.\n",
    "# p-value: 0.08 ‚Üí Indicates the DAG fits well but is not statistically ‚Äúperfect‚Äù.\n",
    "# ‚úÖ Interpretation: Your DAG is statistically valid under this test.\n",
    "# The model is not strongly rejected, meaning your assumed causal directions are likely correct.\n",
    "# Since the significance threshold was 0.2 (a common setting), your DAG passes the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================= #\n",
    "# Attribution Analysis              #\n",
    "# ================================= #\n",
    "#%%\n",
    "# Define the anomaly (a revenue drop)\n",
    "# anomalous_point = {\"sales\": df[\"sales\"].iloc[725]}  # Selecting a known anomalous drop\n",
    "anomalous_point = {\n",
    "    \"sales\": df[\"sales\"].iloc[725],\n",
    "    \"price\": df[\"price\"].iloc[725],\n",
    "    \"ad_spend\": df[\"ad_spend\"].iloc[725],  # Ensure this is included\n",
    "    \"competitor_price\": df[\"competitor_price\"].iloc[725],\n",
    "}\n",
    "\n",
    "# Compute confidence intervals for anomaly attributions\n",
    "median_attributions, confidence_intervals = gcm.confidence_intervals(\n",
    "   gcm.fit_and_compute(\n",
    "       gcm.attribute_anomalies,    # Function to attribute anomalies\n",
    "       model,                        # Structural causal model\n",
    "       bootstrap_training_data=df, # Training data for bootstrapping\n",
    "       target_node=\"sales\",      # Target variable for anomaly attribution\n",
    "       anomaly_samples=pd.DataFrame([anomalous_point])  # The anomaly to analyze\n",
    "   ),\n",
    "   num_bootstrap_resamples=10  # Number of bootstrap resamples for confidence intervals\n",
    ")\n",
    "#%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================= #\n",
    "# Visualize Attributions            #\n",
    "# ================================= #\n",
    "#%%\n",
    "# Helper function to plot attributions\n",
    "def bar_plot(median_attributions, confidence_intervals, title):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    \n",
    "    # Extract the lower and upper bounds of the confidence intervals\n",
    "    lower_bounds = {key: confidence_intervals[key][0] for key in confidence_intervals}\n",
    "    upper_bounds = {key: confidence_intervals[key][1] for key in confidence_intervals}\n",
    "    \n",
    "    # Convert dict_values to NumPy arrays\n",
    "    median_values = np.array(list(median_attributions.values()))\n",
    "    lower_values = np.array(list(lower_bounds.values()))\n",
    "    upper_values = np.array(list(upper_bounds.values()))\n",
    "    \n",
    "    # Calculate the error bars\n",
    "    yerr = [\n",
    "        median_values - lower_values,\n",
    "        upper_values - median_values\n",
    "    ]\n",
    "    \n",
    "    plt.bar(median_attributions.keys(), median_values, yerr=yerr, capsize=5, color=\"royalblue\", alpha=0.7)\n",
    "    plt.xlabel(\"Factors\")\n",
    "    plt.ylabel(\"Anomaly Attribution Score\")\n",
    "    plt.title(title)\n",
    "    plt.xticks(rotation=30)\n",
    "    plt.show()\n",
    "\n",
    "# Plot the anomaly attribution scores with confidence intervals\n",
    "bar_plot(median_attributions, confidence_intervals, \"Revenue Anomaly Attribution\")\n",
    "#%%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
