{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hierarchical Bayesian Model for Price Elasticity\n",
    "\n",
    "üìå **Objective**:\n",
    "- Create a synthetic dataset for a retail pricing problem where we analyze the causal effects of a price change on customer demand\n",
    "- Understand relationship between product price, advertising spend, competitor pricing, and customer demand\n",
    "\n",
    "üîç **Key Takeaways**:\n",
    "- **BLAH**: \n",
    "- **Next Steps**: \n",
    "    - \n",
    "\n",
    "üìå **Methodology**:\n",
    "1. **Create a causal graph** using NetworkX\n",
    "2. **Perform causal discovery** using DoWhy\n",
    "\n",
    "\n",
    "‚úç **Author**: Justin Wall\n",
    "üìÖ **Date**: 02/13/2025\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>customer_segment</th>\n",
       "      <th>region</th>\n",
       "      <th>season</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43.708611</td>\n",
       "      <td>Senior</td>\n",
       "      <td>South</td>\n",
       "      <td>Summer</td>\n",
       "      <td>6.406332e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95.564288</td>\n",
       "      <td>Middle-aged</td>\n",
       "      <td>West</td>\n",
       "      <td>Spring</td>\n",
       "      <td>3.292181e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75.879455</td>\n",
       "      <td>Senior</td>\n",
       "      <td>North</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2.753521e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>63.879264</td>\n",
       "      <td>Senior</td>\n",
       "      <td>West</td>\n",
       "      <td>Summer</td>\n",
       "      <td>5.099610e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.041678</td>\n",
       "      <td>Young</td>\n",
       "      <td>South</td>\n",
       "      <td>Summer</td>\n",
       "      <td>1.379310e-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       price customer_segment region  season         sales\n",
       "0  43.708611           Senior  South  Summer  6.406332e-06\n",
       "1  95.564288      Middle-aged   West  Spring  3.292181e-07\n",
       "2  75.879455           Senior  North  Summer  2.753521e-07\n",
       "3  63.879264           Senior   West  Summer  5.099610e-07\n",
       "4  24.041678            Young  South  Summer  1.379310e-05"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ================================= #\n",
    "# Generate Fake Retail Pricing Data #\n",
    "# ================================= #\n",
    "#%%\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define possible values for hierarchies\n",
    "customer_segments = [\"Young\", \"Middle-aged\", \"Senior\"]\n",
    "regions = [\"North\", \"South\", \"East\", \"West\"]\n",
    "seasons = [\"Winter\", \"Spring\", \"Summer\", \"Fall\"]\n",
    "\n",
    "# Generate synthetic dataset\n",
    "n_samples = 1000  # Number of observations\n",
    "\n",
    "data = {\n",
    "    \"price\": np.random.uniform(10, 100, n_samples),  # Random prices between $10 and $100\n",
    "    \"customer_segment\": np.random.choice(customer_segments, n_samples),\n",
    "    \"region\": np.random.choice(regions, n_samples),\n",
    "    \"season\": np.random.choice(seasons, n_samples)\n",
    "}\n",
    "\n",
    "# Define base demand and elasticity per segment, region, and season\n",
    "segment_elasticity = {\"Young\": -2.0, \"Middle-aged\": -1.5, \"Senior\": -1.2}\n",
    "region_elasticity = {\"North\": -1.8, \"South\": -1.3, \"East\": -1.5, \"West\": -1.6}\n",
    "season_elasticity = {\"Winter\": -1.4, \"Spring\": -1.7, \"Summer\": -2.0, \"Fall\": -1.5}\n",
    "\n",
    "# Compute sales based on price elasticity and some random noise\n",
    "sales = []\n",
    "for i in range(n_samples):\n",
    "    base_demand = 500  # Base demand level\n",
    "    segment = data[\"customer_segment\"][i]\n",
    "    region = data[\"region\"][i]\n",
    "    season = data[\"season\"][i]\n",
    "    price = data[\"price\"][i]\n",
    "\n",
    "    # Compute sales using price elasticity\n",
    "    elasticity = (segment_elasticity[segment] +\n",
    "                  region_elasticity[region] +\n",
    "                  season_elasticity[season] +\n",
    "                  np.random.normal(0, 0.2))  # Adding some noise\n",
    "\n",
    "    predicted_sales = base_demand * (price ** elasticity)  # Simple demand function\n",
    "    sales.append(max(0, predicted_sales))  # Ensure sales are non-negative\n",
    "\n",
    "# Add sales to dataset\n",
    "data[\"sales\"] = sales\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display first few rows\n",
    "df.head()\n",
    "#%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================= #\n",
    "# Create Features for Model         #\n",
    "# ================================= #\n",
    "#%%\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "\n",
    "# Encode categorical variables\n",
    "df[\"customer_segment_code\"] = df[\"customer_segment\"].astype(\"category\").cat.codes\n",
    "df[\"region_code\"] = df[\"region\"].astype(\"category\").cat.codes\n",
    "df[\"season_code\"] = df[\"season\"].astype(\"category\").cat.codes\n",
    "\n",
    "# Standardize price for better numerical stability\n",
    "df[\"price_std\"] = (df[\"price\"] - df[\"price\"].mean()) / df[\"price\"].std()\n",
    "\n",
    "# Log-transform sales to handle large variations and skewness\n",
    "df[\"log_sales\"] = np.log1p(df[\"sales\"])\n",
    "#%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [mu_segment, sigma_segment, mu_region, sigma_region, mu_season, sigma_season, segment_effects, region_effects, season_effects, beta_price, sigma]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97794b159f914aecab44cd4ca833c064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Not enough samples to build a trace.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m     sales_obs \u001b[38;5;241m=\u001b[39m pm\u001b[38;5;241m.\u001b[39mNormal(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msales_obs\u001b[39m\u001b[38;5;124m\"\u001b[39m, mu\u001b[38;5;241m=\u001b[39mmu, sigma\u001b[38;5;241m=\u001b[39msigma, observed\u001b[38;5;241m=\u001b[39mdf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_sales\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;66;03m# Sample from the posterior\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m     trace \u001b[38;5;241m=\u001b[39m \u001b[43mpm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtune\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_accept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inferencedata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m#%%\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wallj\\anaconda3\\envs\\pymc_env\\Lib\\site-packages\\pymc\\sampling\\mcmc.py:935\u001b[0m, in \u001b[0;36msample\u001b[1;34m(draws, tune, chains, cores, random_seed, progressbar, progressbar_theme, step, var_names, nuts_sampler, initvals, init, jitter_max_retries, n_init, trace, discard_tuned_samples, compute_convergence_checks, keep_warning_stat, return_inferencedata, idata_kwargs, nuts_sampler_kwargs, callback, mp_ctx, blas_cores, model, compile_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m    931\u001b[0m t_sampling \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t_start\n\u001b[0;32m    933\u001b[0m \u001b[38;5;66;03m# Packaging, validating and returning the result was extracted\u001b[39;00m\n\u001b[0;32m    934\u001b[0m \u001b[38;5;66;03m# into a function to make it easier to test and refactor.\u001b[39;00m\n\u001b[1;32m--> 935\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_sample_return\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    936\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    937\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    938\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtune\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtune\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    939\u001b[0m \u001b[43m    \u001b[49m\u001b[43mt_sampling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt_sampling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    940\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdiscard_tuned_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiscard_tuned_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_convergence_checks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_convergence_checks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    942\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_inferencedata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_inferencedata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_warning_stat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_warning_stat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[43m    \u001b[49m\u001b[43midata_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midata_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\wallj\\anaconda3\\envs\\pymc_env\\Lib\\site-packages\\pymc\\sampling\\mcmc.py:968\u001b[0m, in \u001b[0;36m_sample_return\u001b[1;34m(run, traces, tune, t_sampling, discard_tuned_samples, compute_convergence_checks, return_inferencedata, keep_warning_stat, idata_kwargs, model)\u001b[0m\n\u001b[0;32m    966\u001b[0m \u001b[38;5;66;03m# Pick and slice chains to keep the maximum number of samples\u001b[39;00m\n\u001b[0;32m    967\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m discard_tuned_samples:\n\u001b[1;32m--> 968\u001b[0m     traces, length \u001b[38;5;241m=\u001b[39m \u001b[43m_choose_chains\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraces\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtune\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    970\u001b[0m     traces, length \u001b[38;5;241m=\u001b[39m _choose_chains(traces, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\wallj\\anaconda3\\envs\\pymc_env\\Lib\\site-packages\\pymc\\backends\\base.py:624\u001b[0m, in \u001b[0;36m_choose_chains\u001b[1;34m(traces, tune)\u001b[0m\n\u001b[0;32m    622\u001b[0m lengths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(trace) \u001b[38;5;241m-\u001b[39m tune) \u001b[38;5;28;01mfor\u001b[39;00m trace \u001b[38;5;129;01min\u001b[39;00m traces]\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(lengths):\n\u001b[1;32m--> 624\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot enough samples to build a trace.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    626\u001b[0m idxs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(lengths)\n\u001b[0;32m    627\u001b[0m l_sort \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(lengths)[idxs]\n",
      "\u001b[1;31mValueError\u001b[0m: Not enough samples to build a trace."
     ]
    }
   ],
   "source": [
    "# ================================= #\n",
    "# Run Hierarchical Bayesian Model   #\n",
    "# ================================= #\n",
    "#%%\n",
    "# Build hierarchical Bayesian model\n",
    "with pm.Model() as hierarchical_model:\n",
    "    # Hyperpriors for group-level effects\n",
    "    mu_segment = pm.Normal(\"mu_segment\", mu=0, sigma=1)\n",
    "    sigma_segment = pm.Exponential(\"sigma_segment\", 1.0)\n",
    "\n",
    "    mu_region = pm.Normal(\"mu_region\", mu=0, sigma=1)\n",
    "    sigma_region = pm.Exponential(\"sigma_region\", 1.0)\n",
    "\n",
    "    mu_season = pm.Normal(\"mu_season\", mu=0, sigma=1)\n",
    "    sigma_season = pm.Exponential(\"sigma_season\", 1.0)\n",
    "\n",
    "    # Group-level effects\n",
    "    segment_effects = pm.Normal(\"segment_effects\", mu=mu_segment, sigma=sigma_segment, shape=len(customer_segments))\n",
    "    region_effects = pm.Normal(\"region_effects\", mu=mu_region, sigma=sigma_region, shape=len(regions))\n",
    "    season_effects = pm.Normal(\"season_effects\", mu=mu_season, sigma=sigma_season, shape=len(seasons))\n",
    "\n",
    "    # Price elasticity coefficient\n",
    "    beta_price = pm.Normal(\"beta_price\", mu=-1.5, sigma=0.5)  # Prior centered around typical elasticity\n",
    "\n",
    "    # Likelihood model\n",
    "    mu = (\n",
    "        segment_effects[df[\"customer_segment_code\"].values]\n",
    "        + region_effects[df[\"region_code\"].values]\n",
    "        + season_effects[df[\"season_code\"].values]\n",
    "        + beta_price * df[\"price_std\"].values\n",
    "    )\n",
    "\n",
    "    sigma = pm.Exponential(\"sigma\", 1.0)\n",
    "    sales_obs = pm.Normal(\"sales_obs\", mu=mu, sigma=sigma, observed=df[\"log_sales\"].values)\n",
    "\n",
    "    # Sample from the posterior\n",
    "    trace = pm.sample(2000, tune=1000, target_accept=0.9, return_inferencedata=True)\n",
    "#%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================= #\n",
    "# Summarize Model Results           #\n",
    "# ================================= #\n",
    "#%%\n",
    "# Summarize results\n",
    "summary = az.summary(trace, var_names=[\"beta_price\", \"segment_effects\", \"region_effects\", \"season_effects\"])\n",
    "summary\n",
    "#%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
